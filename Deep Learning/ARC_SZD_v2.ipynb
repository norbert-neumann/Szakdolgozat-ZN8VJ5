{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNQVLd4q4YzmstRLqLLHJMK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import os\n","from PIL import Image\n","import numpy as np\n","import json\n","import random\n","import torch"],"metadata":{"id":"d7K3oAsPqYDp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ARC_Task:\n","  def __init__(self, inputs, outputs, test_input, test_output):\n","    self.inputs = inputs\n","    self.outputs = outputs\n","    self.test_input = test_input\n","    self.test_output = test_output"],"metadata":{"id":"Kr2EnRifqX3H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from matplotlib.pyplot import imshow\n","%matplotlib inline\n","\n","def visualize(tensor):\n","  #torch.argmax(tensor.reshape(30, 30, 11)).item()\n","  arr = np.zeros((30, 30))\n","  for x in range(30):\n","    for y in range(30):\n","      arr[x, y] = torch.argmax(tensor[x, y]).item()\n","  convert(arr)\n","\n","def visualize2(tensor):\n","  arr = np.zeros((30, 30))\n","  for x in range(30):\n","    for y in range(30):\n","      difs = [torch.abs(v - tensor[x, y]) for v in color2vector]\n","      sums = [torch.sum(difs[i]) for i in range(11)]\n","      index_min = min(range(len(sums)), key=sums.__getitem__)\n","      #print(index_min)\n","      arr[x, y] = index_min\n","  convert(arr)\n","\n","converter = {}\n","converter[0] = np.array([0, 0, 0])\n","converter[1] = np.array([0, 116, 217])\n","converter[2] = np.array([255, 65, 54])\n","converter[3] = np.array([46, 204, 64])\n","converter[4] = np.array([255, 220, 0])\n","converter[5] = np.array([170, 170, 170])\n","converter[6] = np.array([240, 18, 190])\n","converter[7] = np.array([255, 113, 27])\n","converter[8] = np.array([127, 219, 255])\n","converter[9] = np.array([135, 12, 37])\n","converter[10] = np.array([255, 255, 255])\n","\n","def convert(X):\n","  Y = np.zeros(shape=(X.shape[0], X.shape[1], 3), dtype=np.uint8)\n","  for x in range(X.shape[0]):\n","    for y in range(X.shape[1]):\n","      Y[x, y] = converter[X[x, y]]\n","  imshow(Y)"],"metadata":{"id":"4x5xt2W3TXSy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def fetch():\n","  data = []\n","  MAX_LENGTH = 30\n","  files = [f for f in os.listdir() if f.endswith('json')]\n","  for f in files:\n","    with open(f, 'r') as file:\n","      json_file = json.loads(file.read())\n","      inputs = [one_hot_encode(np.array(train_exmaples['input'])) for train_exmaples in json_file['train']]\n","      outputs = [one_hot_encode(np.array(train_exmaples['output'])) for train_exmaples in json_file['train']]\n","      test_input = [one_hot_encode(np.array(train_exmaples['input'])) for train_exmaples in json_file['test']]\n","      test_output = [one_hot_encode(np.array(train_exmaples['output'])) for train_exmaples in json_file['test']]\n","      data.append(ARC_Task(inputs, outputs, test_input[0], test_output[0]))\n","\n","  return data\n","\n","def one_hot_encode(array):\n","  MAX_LENGTH = 30\n","  arr = torch.zeros((MAX_LENGTH, MAX_LENGTH, 11))\n","  size = array.shape\n","  for x in range(MAX_LENGTH):\n","    for y in range(MAX_LENGTH):\n","      if x < size[0] and y < size[1]:\n","        arr[x, y, array[x, y]] = 1\n","      else:\n","        arr[x, y, 10] = 1\n","  return arr.flatten()"],"metadata":{"id":"wjy6VS5nqXpZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset\n","\n","class ARCDataset(Dataset):\n","  def __init__(self, data):\n","    self.data = data\n","\n","  def __getitem__(self, idx):\n","    return self.data[idx]\n","\n","  def __len__(self):\n","    return len(self.data)"],"metadata":{"id":"9aSpn74dqXTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MEjhg0jAodAk"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Autoencoder(nn.Module):\n","\n","  def __init__(self, input_shape=30*30*11, latent_dim = 128):\n","    super().__init__()\n","    self.input_shape = input_shape\n","    self.latent_dim = latent_dim\n","    self.encoder_l1 = nn.Linear(self.input_shape, self.latent_dim, bias=True)\n","    self.drop1 = nn.Dropout(0.5)\n","    self.encoder_l2 = nn.Linear(self.latent_dim, self.latent_dim, bias=True)\n","    self.decoder_l1 = nn.Linear(self.latent_dim, self.latent_dim, bias=True)\n","    self.drop2 = nn.Dropout(0.5)\n","    self.decoder_l2 = nn.Linear(self.latent_dim, self.input_shape, bias=True)\n","\n","  def forward(self, task):\n","    task_vector = torch.zeros(self.latent_dim)\n","    for input, output in zip(task.inputs, task.outputs):\n","      latent_input = model.run_encoder(input)\n","      latent_output = model.run_encoder(output)\n","      task_vector += latent_output - latent_input\n","    task_vector /= len(task.inputs)\n","    latent_prediction = model.run_encoder(task.test_input) + task_vector\n","    prediction = model.run_decoder(latent_prediction)\n","    return prediction\n","\n","  def run_encoder(self, x):\n","    output = self.drop1(F.sigmoid(self.encoder_l1(x)))\n","    latent = F.sigmoid(self.encoder_l2(output))\n","    return latent\n","\n","  def run_decoder(self, latent):\n","    output = self.drop2(F.sigmoid(self.decoder_l1(latent)))\n","    x_hat = F.sigmoid(self.decoder_l2(output))\n","    return x_hat"]},{"cell_type":"code","source":["train_dataset = ARCDataset(fetch())\n","len(train_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MVFAMSdyrEqu","executionInfo":{"status":"ok","timestamp":1664099063652,"user_tz":-120,"elapsed":25351,"user":{"displayName":"Norbert Neumann","userId":"16010244964456960411"}},"outputId":"760aa133-fbb6-434d-d2fa-eb824b8efe6c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["400"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["model = Autoencoder(latent_dim=400)\n","model.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XmK2w6GYrMKG","executionInfo":{"status":"ok","timestamp":1664099069681,"user_tz":-120,"elapsed":387,"user":{"displayName":"Norbert Neumann","userId":"16010244964456960411"}},"outputId":"0c3cab03-c7de-4ecd-8225-08826205159a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Autoencoder(\n","  (encoder_l1): Linear(in_features=9900, out_features=400, bias=True)\n","  (drop1): Dropout(p=0.5, inplace=False)\n","  (encoder_l2): Linear(in_features=400, out_features=400, bias=True)\n","  (decoder_l1): Linear(in_features=400, out_features=400, bias=True)\n","  (drop2): Dropout(p=0.5, inplace=False)\n","  (decoder_l2): Linear(in_features=400, out_features=9900, bias=True)\n",")"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["import torch.optim as optim\n","\n","mse = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)\n","n_epochs = 30\n","\n","for epoch in range(n_epochs):\n","  loss = 0\n","\n","  for task in train_dataset:\n","    #task = task.cuda()\n","    optimizer.zero_grad()\n","    prediction = model(task)\n","    train_loss = mse(prediction, task.test_output)\n","    train_loss.backward()\n","    optimizer.step()\n","    loss += train_loss.item()\n","\n","  if epoch % 5 == 0:\n","    print(epoch, loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XfjBQDkksDJ9","executionInfo":{"status":"ok","timestamp":1664104218955,"user_tz":-120,"elapsed":2170450,"user":{"displayName":"Norbert Neumann","userId":"16010244964456960411"}},"outputId":"11c49e67-7a45-4b98-d1c6-452e97e53e9b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 4.913577297586016\n","5 4.779691016417928\n","10 4.685127571108751\n","15 4.649243375228252\n","20 4.501520568388514\n","25 4.4617546204826795\n"]}]},{"cell_type":"code","source":["p = model.forward(train_dataset[145])\n","visualize(p.reshape(30, 30, 11))"],"metadata":{"id":"I_rO_B9jsJWm","executionInfo":{"status":"ok","timestamp":1664101635425,"user_tz":-120,"elapsed":321,"user":{"displayName":"Norbert Neumann","userId":"16010244964456960411"}},"colab":{"base_uri":"https://localhost:8080/","height":266},"outputId":"75cefac7-312b-4e0a-8705-5e92624bcaa2"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]}]}